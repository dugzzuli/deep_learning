# deeplearning
=============================================================================================================
# 1.在卷积神经网络中,经过卷积层或池化层后还剩多少像素：      
这里不考虑边缘填充的问题，只讲方法帮助理解以便笔试中快速计算。
【图片尺寸-卷积核尺寸(或者池化尺寸)】/步长+1=卷积或者池化
后的尺寸....最后加1是因为比如1,2有两个数，2-1+1=2。。。      
例如:图片尺寸100*100,卷积核尺寸5*5,【按照卷积核窗口的最后一个像素为标准移动,一直
移动到图片的最后一个像素】      
   【步长为1时】，经过卷积层后的像素大小计算过程为，由于卷积核为5*5,那么图片的
		前5*5个像素是第1个点，然后向右移动一步则第2个到第6个像素的5*5个像素变成第2个点，
		以此类推,从第5个像素移动到第100个像素共有100-5+1=96个,即卷积层后图片变为了96*96,这是步长为1的情况。        
   【步长为2时】：是第5,7,9,11,...,99。总共移动了(99-5)/2+1=48次，即图片由100*100变为了48*48了。
		这里没考虑第100个像素，因为步长为2，最后只剩一列了，组合不了5*5，只有4*5.填充什么的我不考虑。     
   【步长为3时】，按照上面的方法就行了。5,8,11,...,98。。。核窗口的最后一个像素总共移
		动了(98-5)/3+1=32次，卷积或者池化后的尺寸为32*32     
	对于书中讲的【池化层】，比如2*2的池化窗口并没有重叠的，那是因为池化的步长为2.
	如果池化窗口为3*3,步长为3，那也是没有重叠的.
==============================================================================================================
# 2.关于权重初始化是否可为0的问题      
【1.神经网络】：隐藏层的每个神经元分别学习到不同的特征，如果权重全部初始化为0，那么
	各层的每个神经元都学习相同的特征，就相当于每层神经元都只有一个神经元，因此神经网络不能权重初始化为0.    
【2.单层感知机】只有一个神经元，他的权重可初始化为0      
【3.线性回归模型】相当于单层感知机，其权重也可以初始化为0

==============================================================================================================
# 3.关于梯度下降法        
【1】比如[x1 x2] = meshgrid(-5:5, -5:5);y=x1.^2+x2.^2  surf(x1,x2,y); 可把该代码在matlab中运行看图形是什么样。       
	这个表达式的图形就是一个把四个角吊起来的网。梯度为(2*x1,2*x2),该图的负梯度方向是平行于x1 x2平面
	并指向里面的方向，并不是该点的切线方向，也不是某个指向地面的容易让人混淆的切线.      
	
【2】但是对于只有一个变量的比如 y=x.^2,其梯度为2*x，即其负梯度方向为-2*x，即是其导数。
	对于线性函数比如y=x,其梯度就是其斜率。      

【3】#只需要记住，负梯度方向是使函数值减少最快的方向，但是是供变量迭代相加减的，并不是供函数值相加减的。
	只需要用变量加上负梯度方向就是变量的下一个迭代值:w1=w0-α*dy/dw0,α是步长,dy/dw0是在w0处的梯度方向。     
	
【4】在多变量时候还容易想通，对于一元二次函数，对变量迭代时，由于根据图像来推导，总是觉得梯度方向是跟
	变量迭代的加减扯不上关系的，不知道为什么要用变量加上步长乘以负梯度。似懂非懂，那就不管了吧，记住多变量的就好
	
==============================================================================================================

# 4.logistic回归--------(摘自吴恩达深度学习视频)
	【0】logistic回归是二类分类问题，对应标签y为0和1。
		激活函数是sigmoid函数, a=σ(z)
		假设只有每个样本只有两个特征x1,x2：
			z=w(T)x + b = w1*x1+w2*x2+b,  
	【编程时有种约定】da表示dL(a,y)/da ;dz表示dL(a,y)/dz     也就是说，把dL/省略了

	【1】单个样本损失函数：L(a,y)=-[ylog(a)+(1-y)log(1-a)] ,#【这个公式很重要，记清楚】
		其中a是预测值、激活值,a属于0到1之间的值,也就是y帽,y是标签。      
		
	【1.0】L(a,y)对a求导得：	
			dL(a,y)/da = -y/a + (1-y)/(1-a);#【这个公式很重要，记清楚】  

			dL(a,y)/dz = a-y;     #【这个公式很重要，记清楚】  
			# 证明：
			# 因为dL/dz = (dL/da)(da/dz),	
			# 又公式σ'(z)=σ(z)*[1-σ(z)]		#【这个公式很重要，记清楚,橙色深度学习上有推导】
			# 所以dL(a,y)/dz = a-y    
			
			dL/dw1 = (dL/dz)(dz/dw1) =x1*(dL/dz);
			dL/dw2 = x2*(dL/dz);(同理)
			dL/db  = dL/dz       
			
		编程中会把上面的式子这么表示：
		dw1 = dL/dw1 = x1*dz;
		dw2 = dL/dw2 = x2*dz;
		 db = dL/db  = dz	【这三行行中的dz表示dL(a,y)/dz】
		
		【重点来了】求出了上面的这些式子，就可以求出单个样本的一次梯度迭代(更新)了：
		w1 = w1-α*dw1			即[w1 = w1-α*x1*(a-y)]
		w2 = w2-α*dw2			即[w2 = w2-α*x2*(a-y)]
		 b = b -α*db			即[ b = b -α*(a-y)]
		   
	【1.1】当标签y为0时,损失函数为-log(1-a),为使损失函数越小,就必须使a无限接近0。
			即标签为0时，让损失函数越小会使得其预测值(激活值)会趋向于等于0，正是趋向于正确答案的操作。           
	【1.2】当标签y为1时,损失函数为-log(a)  ,为使损失函数越小,就必须使a无限接近1。
			即标签为1时，让损失函数越小会使得其预测值(激活值)会趋向于等于1，正是趋向于正确答案的操作。
			
	【2】总体成本函数：
			J(w,b)=-1/m *Σ[y(i)log(a(i))+(1-y(i))log(1-a(i))],即对m个样本求和后再平均，Σ是求和符号  
			也即J(w,b)=-1/m *ΣL(a(i),y)		其中a=σ(z)，a(i)=σ(z(i))是第i个样本的激活值(预测值)
			
	【3.1】损失函数：是衡量单一训练样本的效果      
	【3.2】成本函数：用于衡量参数w和b的效果,他是在全部训练集上来衡量

	【4】编程代码：【伪代码，不能运行，只是讲解帮助理解】
		# 以只有两个特征的样本为例：
	J=0
	dw1=0
	dw2=0
	db=0
	for i in range(1,m+1):	#遍历m个样本	#视频中的写法是 for i=1 to m 
		z[i]=w(t)*x[i]		#w(t)表示w的转置，x[i]表示第i个样本，不是滴i个特征
		a[i]=σ(z[i])
		J+=-y[i]log(a[i])+(1-y[i])log(1-a[i])
		dz[i]=a[i]-y[i]
		# 由于只有两个特征，下面的几行代码对特征不需要使用循环遍历，假如很多特征，就需要使用循环遍历了。
		# 但后面讲了for循环效率低下，要使用向量化方式
		dw1+=x1[i]dz[i]
		dw2+=x2[i]dz[i]
		db +=dz[i]
	J/=m	#即把m个损失函数之和也就是成本函数，除以m个样本
	dw1/=m 
	dw2/=m
	db/=m

	dw1=fy(j)/fy(w1)  #fy是求偏导符号，我自己简写的
	w1 = w1-α*dw1		
	w2 = w2-α*dw2			
	b  = b -α*db
	# 以上代码只对所有样本进行了一次迭代

	【5】疑问
	【5.1】可能会有的疑问：求出了成本函数J，为什么没有最小化成本函数J?
		答：最小化成本函数J的方法就是梯度下降法，上面的代码就是迭代就是在运用梯度下降法。

	【5.2】可能会有的疑问：单个样本的一次梯度迭代更新与所有样本的一次梯度迭代更新有什么不同？
		答：首先说明梯度指的是损失函数或者成本函数对每个权重求偏导数。每个特征对应一个权重（神经网络除外）。
		权重更新是每个权重分别更新，即w1,或者w2,...wn分别使用其对应的负梯度更新。
		所有样本的梯度迭代更新会用到单个样本的梯度，即所有样本的梯度是（所有单个样本的梯度和，除以样本个数m）。
		之所以单个样本讲到梯度更新是为了讲明白原理。当实际编程时，都是用到的所有样本的的梯度，
		单个样本梯度不更新，只相加得到所有样本总梯度，不会求单个样本的梯度更新的，
		即梯度更新是发生在求所有样本梯度和再除以样本总数m之后。
		【因此，在吴恩达视频中，Z=[z(1), z(1), ..., z(1)]=[w(T)x(1)+b, w(T)x(2)+b, ..., w(T)x(m)+b] 
		才会出现w(T)和所有样本都相乘，却并非每个样本的w(T)都不一样，因为所有样本都计算完毕才做一次迭代。】
		'''
		以下内容可不看，直接看"logistic回归_源代码.py"文件中的【0个显示for循环结构】
		X样本集，由所有样本以行向量形式组成，即每列是一个样本，每行是特征。
		Y是标签，是行向量，每个元素对应各个样本的标签。Y=[y(1),y(2),y(3),...,y(m)]
		W是权重，是n维列向量，n表示每个样本有n个特征
		Z=np.dot(w(T),X),
		a=σ(z),A=σ(Z)即A=[a(1),a(2),...,a(m)]即每个样本的预测值(激活值)组成了A，然后梯度更新会用到A，
		例如由单个非向量形式w1 = w1-α*x1*(a-y)，可知向量化形式为W = W - *X*(A-Y)，错误，
		正确方法应该用到hadamard乘积⊙，即每列对应元素相乘：W=W-α*X(T)⊙(A-Y)(T)。权重更新是每个权重分别更新
		'''
	=========================================================================================================
	【6】向量化
	向量化通常是消除你的代码中显示for循环语句的艺术
	关于向量化的例子和源代码，见 "dot_shape_reshape_向量化与for循环.py" 文件

	=========================================================================================================

	【7】np.dot()内积函数的用法
	【7.1】求向量内积时，函数np.dot(a,b)=a*b,向量a和向量b都不需要做任何转置，
		前提是：向量a和向量b其中某一个为行向量,另一个为列向量，且向量a和向量b的维度一致。
		当行*列，结果就是一个1*1矩阵
		当列*行，结果就是一个n*n矩阵

	【7.2】当a，b为矩阵时，a的列数要等于b的行数

	=========================================================================================================

	【8】broadcasting 广播
	 简单来说，广播就是使得
	 1.向量加减乘除常数时，把常数复制成向量的形式，使得他们可以相加减乘除  
		例如[1,2,3]+1=[1,2,3]+[1,1,1]=[2,3,4]
	 2.矩阵加减乘除向量时，把向量复制成矩阵的形式，使得他们可以相加减乘除	
		例如[[1,2,3],[4,5,6]]+[1,2,2]=[[1,2,3],[4,5,6]]+[[1,2,2],[1,2,2]]
	 3.。。。。
	----------------------------------------------------------------------
	import numpy as np
	A=np.array([[56.0, 0.0, 4.4, 68.0],
			   [1.2, 104.0, 52.0, 8.0],
			   [1.8, 135.0, 99.0, 0.9]])		   
	print('A=',A)

	cal=A.sum(axis=0)		#axis=0表示对矩阵A的每列求和；axis=1表示对矩阵A的每行求和
	print(cal)

	percentage=100*A/cal.reshape(1,4)	#reshape只是为了确保cal是1*4的矩阵，如果cal是，可不加，不是就加上。是不是都可以加上。
	print(percentage)
	--------------------------------------------------
	--------------------------------------------------
	# 当需要定义一维行向量和列向量时，务必指定行数和列数，避免生成既非行向量也非列向量的秩为1的数组
	# 如果你生成了秩为1的数组，可以使用a.reshape()函数来转换为行向量或者列向量
	# 注意维度问题，用array生成数组时，记得需要多打一对中括号[]把数组内容括起来
	# 可以使用断言assert语句来保证不生成秩为1的数组：
	a=np.array([1,2,3,4,5])		#(5,)
	assert a.shape==(1,5),'数组a不是(1,5)维的数组'		# AssertionError:数组a不是(1,5)维的数组
	
	a=np.array([[1,2,3,4,5]])
	assert a.shape==(1,5)，'数组a不是(1,5)维的数组'		#正常运行，不会出现AssertionError提示
	
	import numpy as np
	a=np.random.randn(5)	#生成5个随机高斯变量，储存在数组a中
	print(a)
	print(a.shape)		#(5,),秩为1的数组，他既不是行向量也不是列向量
	print(a.T)			#转置后也不变，还是秩为1的数组
	print(np.dot(a,a.T))

	a=np.random.randn(5,1)	#5*1的随机正态分布数组，列向量
	print(a.T)

	# 注意维度问题，用array生成数组时，记得需要多打一对中括号[]把数组内容括起来
	a=np.array([1,2,3])		#a.shape (3, ) 秩为1的数组
	b=np.array([[1,2,3]])	#b.shape (1, 3)
=================================================================================================
	【9】logistic损失函数与成本函数
	(y'和y"都表示预测值,也可就是激活值a，就是视频中的y帽)因为单引号双引号同一行换着使用才不会配对而把内容引起来
	----------------------------------------------
	【损失函数：】
	if y=1:	p(y|x)=y"	
	if y=0:	p(y|x)=1-y"
	则上两式子可合并为
		p(y|x)=y"^y * (1-y')^(1-y)

	log(p(y|x))=ylog(y') + (1-y)log(1-y")
			   =-L(y",y)
	即损失函数为
	L(a,y)= - [ylog(y') + (1-y)log(1-y")]
	因此，最小化损失函数就是最大化log(p(y|x))
	----------------------------------------------
	【成本函数：】
	对于单个样本来说：
		p(y|x)=y"^y * (1-y')^(1-y)
	假设所有样本独立同分布，所有这些样本的联合概率就是每个样本概率的乘积,整个训练集中标签的概率
		p(labels in target set)= ∏ p(y^(i)|x^(i)) # ∏表示求所有样本的乘积
	需要找到一组参数，使得给定样本的观测值概率最大，令这个概率最大化，等价于令其对数最大化：
		log [p(labels in target set)]= log{ ∏ p(y^(i)|x^(i))}
	训练集的标签出现的概率的对数等于
		log [p(labels in target set)]= ∑ log p(y^(i)|x^(i))	#对数()里面的乘积可以变成对数的相加
									 = -∑ L(y"^(i),y^(i))
	这样也就推导出了前面给出的logistic回归的成本函数J(w,b)=1/m *∑ L(y'^(i),y^(i)),这里加上了缩放常数因子1/m
														  =-1/m *∑ log p(y^(i)|x^(i))	
														  =-1/m *∑ log{y"^(i)^y^(i) * (1-y'^(i))^(1-y^(i))}
														  =-1/m *∑ {y^(i)log(y'^(i)) + (1-y^(i))log(1-y"^(i))}
					# 第【2】条里面写的成本函数：   J(w,b)=-1/m *∑[y(i)log(a(i))+(1-y(i))log(1-a(i))]   对上号了
	【最大似然估计】
	最大似然估计是给出了结果，求使得该结果出现可能性最大的一组参数的值。
		本例就是相当于给出J(w,b)，由于J(w,b)带负号，则相当于求使得-J(w,b)最大时
		（即J(w,b)最小时）的参数w,b的值。求最大最小这就变成了凸优化问题了
		更新迭代w,b,求出了最终的w,b那么模型就训练好了











