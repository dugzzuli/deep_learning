
#本周内容：优化算法

机器学习的应用是一个高度依赖经验的过程，伴随着大量迭代过程，你需要训练诸多模型，才能找到合适的那一个，所以，优化算法能够
帮助你快速训练模型，其中一个难点在于，深度学习没有在大数据领域发挥最大的效果，我们可以利用一个巨大的数据集来训练神经网络，
而在巨大数据及基础上进行训练速度很慢，因此，使用快速的好用的优化算法能够大大提高你和团队的效率。
1.【mini-batch梯度下降法】
	向量化能让你有效地对所有m个例子进行计算，允许你处理整个训练集，而无需某个明确的公式。所以我们要把训练样本放到巨大的矩阵X当中去。
	X=[x^(1) x^(2) x^(3) ... x^(m)],m个样本,X的维数是(n_x,m)
	Y=[y^(1) y^(2) y^(3) ... y^(m)],Y的维数是(1,m)
	但是如果样本总数m很大的话，处理速度仍然缓慢，比如说，如果m是500万或者更大的数，在对整个训练集执行梯度下降法时，你必须处理整个
	训练集，然后才能进行一步梯度下降法；然后你需要再重新处理500万个训练样本，才能进行下一步梯度下降。所以如果你在处理完整个500万
	个样本的训练集之前，先让梯度下降法处理一部分，你的算法速度会更快。
	
	你可以把训练集分割为小一点的子训练集，这些子训练集被取名为Mini-batch,假设每一个子集中只有1000个样本，那么把其中的x^(1)到x^(1000)
	取出来，将其称为第一个子训练集，也叫作mini-batch;然后你再取出接下来的1000个样本，从x^(1001)到x^(2000)，然后再取1000个样本；以此类推
	将x^(1)到x^(1000)的样本称为X^{1},x^(1001)到x^(2000)的样本称为X^{2},...,最后是X^{5000}，即总共有5000个mini-batch。
	对Y也要进行相同处理，相应的拆分Y的训练集，y^(1)到y^(1000)的样本称为Y^{1},y^(1001)到y^(2000)的样本称为Y^{2},...,最后是Y^{5000}。
	mini-batch的数量t组成了X^{t}和Y^{t}
	X的维数是(n_x,m)，X^{t}的维数应该是(n_x,1000),Y^{t}的维数都是(1,1000).
	【batch梯度下降法指的是我们之前讲过的梯度下降法，就是同时处理整个训练集。】
	【mini-batch梯度下降法指的是每次同时处理单个mini-batch的X^{t}和Y^{t}。】
	
	while 没达到要求 ：
		for t in range(1,5000):		#注意：Z[1],A[1],...,Z[L],A[L]都是向量。维度为(1,1000)?
		   forwardprop on X^{t}
			Z[1]=W[1]X^{t}+b[1]
			A[1]=g[1](Z[1])
			...
			Z[L]=W[L]X^{t}+b[L]
			A[L]=g[L](Z[L])    		#A[L]是预测值
			
			compute cost J^{t}=1/1000*ΣL(a(i),y(i)) + λ/2m*Σ||w^[l]||F^2
			
			backprop to compute gradients (执行反向传播来计算J^{t}的梯度)：
			W[l]=W[l]-α*dW[l]
			b[l]=b[l]-α*db[l]
	
	如果mini-batch size=m,就是batch梯度下降
	如果mini-batch size=1,就是随机梯度下降
	
	随机梯度下降，每次迭代，你只对一个样本进行梯度下降，大部分时候你向着全局最小值靠近，有时候你会远离最小值，因为那个样本恰好给你
	指的方向不对，因此随机梯度下降是有很多噪声的。平均看来，它最终会靠近最小值，不过有时候也会方向错误，因为随机梯度下降法永远不会
	收敛，而是会一直在最小值附近波动，但它并不会达到最小值并停留在此。
	 实际上你选择的mini-batch size大小在1和m之间,而1太小，m太大了。原因在于使用batch梯度下降法，每次迭代需要处理大量训练样本，该算法
	 的主要弊端在于特别是训练样本数量巨大的时候，单词迭代耗时太长，如果驯良样本不大，batch梯度下降法也能运行的很好。相反，如果使用
	 随机梯度下降法，如果你只要处理一个样本，那这个方法很好，这样做没有问题，通过减小学习率，噪声会被改善或有所减少，但随机梯度下降
	 法的一大缺点是，你会失去所有向量化带给你的加速。因为一次性只处理了一个训练样本，这样效率过于低下。所以实践中最好选择不大不小的
	 mini-batch尺寸，实际上学习率达到最快，你会发现两个好处，一方面你得到了大量向量化，另一方面，你不需要等待整个训练集被处理完，就
	 可以进行后续工作。每代(epoch)训练集允许我们采取5000个梯度下降步骤。
	 怎样选择mini-batch的尺寸？
	 如果样本集少于2000个样本，就直接用batch梯度下降法
	 如果样本数量较大的话，就使用mini-batch梯度下降法。一般的mini-batch尺寸大小为64到512。考虑到电脑内存设置和使用的方式，如果
	 mini-batch大小是2的某次方的时候(64、128、256、512)，代码会运行地更快。
	 X^{t}和Y^{t}要符合cpu/gpu内存，如果你处理的mini-batch和cpu/gpu内存不相符，不管你用什么方法处理数据，你会注意到算法的表现急转直下，
	 变得惨不忍睹。
	 一般需要多试几次mini-batch尺寸的值.
	 
【2】指数加权平均（或：指数加权移动平均：exponentially weighted average）
	θt是第t天的温度，表示第t天的移动平均值vt，即每日温度的指数加权平均值。
	v0=0
	v1=0.9*v0+0.1*θ1
	v2=0.9*v1+0.1*θ2
	v3=0.9*v2+0.1*θ3
	...
	vt=0.9*v(t-1)+0.1*θt
	
	即vt=β*v(t-1)+(1-β)*θt，上面的例子取β=0.9，vt可近似等于 1/(1-β) 天的温度
	例如β=0.9 ，则vt可近似等于 1/(1-0.9) =10 天的温度的平均值
	例如β=0.98，则vt可近似等于 1/(1-0.98)=50 天的温度的平均值
	β取值越大如β=0.98，就表示给前一天的值加了太多权重，只有0.02的权重给了当日的值，所以温度上下起伏变化时，
	当β较大时，指数平均值会适应得更慢一些。
	β=0.5时，只平均了两天的温度，平均的数据太少，所以得到的曲线有更多的噪声，更有可能出现异常值，但是这个曲线可以更快的适应温度变化。
	所以指数加权平均数经常被使用，它在统计学中被称为指数加权移动平均值，我们简称为指数加权平均数。
	通过调整这个参数，或者说后面的算法学习，你会发现这是一个很重要的参数，可以取得稍微不同的效果，往往中间有某个值最好。
	
	实际编程中的做法：
	v=0
	v=0.9*v+0.1*θ1
	v=0.9*v+0.1*θ2
	v=0.9*v+0.1*θ3
	...
	v=0.9*v+0.1*θt
	
	即用v加权后的运算值 把v覆盖
	指数加权平均数公式的好处之一在于：它只占用极少内存，电脑内存只占一行数字而已，然后把最新数据代入公式，不断地覆盖可以了。
	正因为这个原因，其效率，它基本上只占用一行代码，计算加权平均数也只占用单行数字的储存和内存。当然它并不是最好的，也不是最精
	准的仅算平均数的方法。如果你要计算移动窗，你直接计算出过去10天或过去50天的总和，除以10或者50就好，如此往往得到更好的估测。
	但缺点是，如果保存所有最近的温度数据和过去10天的总和，必须占用更多的内存，执行更加复杂，计算成本也更加高昂。

【3】指数加权平均的偏差修正
	看网易云课堂第二课第二周 2.5指数加权平均的偏差修正
【4】Momentum(动量)或Momentum梯度下降法
	我自己的理解就是：之前的梯度下降法就是跟随梯度方向走，而Momentum梯度下降法是有一个指数加权平均，平均多少次迭代的梯度，即微分项
	给了球一个加速度，使得每次迭代不跟着本次梯度方向变化。我就先这么理解着吧。
	
	Momentum梯度下降法 运行速度几乎总是快于标准的梯度下降法。简而言之，基本的想法就是，计算梯度的指数加权平均数，并利用
	该梯度更新你的权重。在本视频中，我们要一起拆解单句描述，看看你到底如何计算。
	例如，你要优化成本函数如图，图是一个很多个椭圆嵌套的图像，最小值在中心。设初始点在靠近横轴、远离纵轴的位置.如果进行梯度下降
	的一次迭代，无论是batch或mini-batch梯度下降法，都会计算很多步迭代，在纵轴方向来回摆动很多次，慢慢摆动到最小值。这种上下波动
	减慢了梯度下降法的速度，你就无法使用更大的学习率。如果你要使用较大的学习率，结果可能会偏离函数的范围。为了避免摆动过大，需要
	使用一个较小的学习率。
	
	另一个看待问题的角度是，在纵轴上，你希望学习慢一点，因为你不想要纵轴方向上的这些摆动，但是在横轴方向上，你希望加快学习，你希望
	在横轴方向上快速移动靠近最小值点。所以使用Momentum梯度下降法，需要做的是，在每次迭代中，确切来说，是在第t次迭代的过程中，你会
	计算微分dw,db。用现有的mini-batch计算dw，db。
	V_dW = β*V_dW + (1-β)*dW
	V_db = β*V_db + (1-β)*db
	更新参数值：
	W = W - α*V_dW
	b = b - α*V_db
	这样就可以减缓梯度下降的幅度，例如，在上几个导数中，如果平均这些梯度，你会发现这些纵轴方向上的摆动平均值接近0，所以在纵轴上，
	你希望放慢一点，平均过程中，正负数相互抵消，所以平均值接近于0。但是在横轴上，所有的微分都指向横轴方向，因为横轴方向的平均值
	仍然较大，因此用算法几次迭代后，你发现Momentum梯度下降法最终纵轴方向的摆动变小了，横轴方向运动更快。因此你的算法走了一条更加
	直接的路径，在抵达最小值的路上减少了摆动。
	
	Momentum的一个本质，就是如果你要最小化碗状函数，他们能够最小化碗状函数，这些微分项（dW,db），想象它们是你从山上往下滚的一个球，
	提供了加速度，Momentum项（V_dW，V_db）就相当于速度。微分给了这个球一个加速度，此时球正往山下滚，球因为加速度越滚越快，因为β稍
	小于1，表现出一些摩擦力，所以球不会无限加速下去，所以【不像梯度下降法每一步都独立于之前的步骤】，你的球可以向下滚，获得动量，
	可以从碗向下加速，获得动量。
	
	最后我们来看看具体如何计算，公式还是上面的四个公式，所以你有两个超参数，学习率α以及参数β。β控制着指数加权平均数，β最常用的值
	是0.9，即我们之前平均了过去十天的温度，所以在这里就是【平均了前十次迭代的梯度】，实际上，β为0.9时效果不错，你可以尝试不同的值，可以
	做一些超参数的研究，不过0.9是很棒的鲁棒数。
	关于偏差修正,你要拿V_dW和V_db除以(1-β^t)。实际上，人们不这么做，因为10次迭代之后，因为你的移动平均已经过了初始阶段，不再是一个
	具有偏差的预测，实际中，在使用梯度下降法或Momentum时，人们不会受到偏差修正的困扰，当然V_dW的初始值是0，要注意到这是和dW拥有相同
	维数的零矩阵，也就是跟W拥有相同的维数，V_db的初始值也是向量0，即V_db和db、b都是同一个维数。
	
	查阅Momentum的时候，可能会看到这样的公式（已经不怎么用到的公式了）：
		V_dW = β*V_dW + dW
		V_db = β*V_db + db
	即去掉了后面的(1-β)。相当于V_dW和V_db缩小了(1-β)倍，相当于乘以1/(1-β)。使用这种公式的时候，要注意α要根据1/(1-β)相应变化。实际上，
	无论用哪种公式，效果都不错，只是会影响到学习率α的最佳值。吴恩达一般只用第一种公式，因为用第二种公式的时候，如果你要调整超参数β，
	就会影响到V_dW，V_db，你也许还要修改学习率。
	
【5】RMSprop:root mean square prop   （均方根prop）
	该算法也可以加速梯度下降。
	之前的梯度下降法，虽然横轴方向在推动，但是纵轴方向会有大幅度摆动。假设纵轴代表参数b，横轴代表参数W，可能有w1和w2或者其他重要
	的参数，为了便于理解，被称为b和W。所以如果你想减缓b方向上的学习，同时加快至少不是减缓横轴方向的学习，RMSprop算法可以实现这一点。
	
	在第t次迭代中，该算法会照常计算当下mini-batch的微分dW、db，所以我会保留这个指数加权平均数。我要用到S_dW、而不是V_dW，因此
		S_dW=β*S_dW + (1-β)*(dW)^2
		S_db=β*S_db + (1-β)*(db)^2
	更新参数值：
		W = W - α*dW/√(S_dW)		，√表示根号
		b = b - α*db/√(S_db)
	原理：记得在横轴方向或者例子中的W方向，我们希望学习速度快，而在纵轴方向（b方向），我们希望减缓纵轴上的摆动，所以有了S_dW、S_db，
	我们希望S_dW会相对较小，所以我们要除以一个较小的数（√(S_dW)）；而希望S_db较大，所以我们要除以一个较大的数字（√(S_db)），这样就
	可以减缓纵轴上的变化。你看这些微分，纵轴方向上的要比水平方向的大得多，所以斜率在b方向特别大，所以这些微分中，dW较小，db较大。
	因为函数的倾斜程度，在纵轴方向上（b方向上）要大于横轴方向（W方向），db的平方较大，所以S_db也会较大；而相比之下dW会小一些，dW的
	平方会小一些，因此S_dW会小一些。结果就是纵轴方向的更新要被一个较大的数相除就能消除摆动；而水平方向的更新，则被较小的数相除。
	RMSprop的影响就是，你的更新最后会是这样：纵轴方向上的摆动较小，而横轴方向继续推进。
	还有个影响就是，你可以用一个更大的学习率α，然后加快学习，而无需在纵轴方向偏离。
	
	要说明一点，我一直把纵轴和横轴方向分别称为b和W,只是为了方便展示而已，实际中，你会处于参数的高维度空间，所以你需要消除摆动的垂直
	维度，实际上是参数W1,W2,Wn的合集，水平维度可能是W3,W4等等，因此把W和b分开只是方便说明，实际中，dW是一个高维度的
	参数向量，db也是一个高维度参数向量，但是你的直觉是，在你要消除摆动的维度中，最终你要计算一个更大的和值，这些平方and微分的加权平
	均值，所以你最后去掉了那些有摆动的方向。这就是RMSprop，全是均方根，因为你将微分进行平方，然后最后使用平方根。

【6】Adam优化算法：Momentum 和 RMSprop结合
	Adam优化算法基本上就是将Momentum 和 RMSprop结合在一起。
	
	细节1：我们在Momentum中采用超参数β，为了避免混淆，我们现在不用β而采用超参数β_2，以保证在Momentum和RMSprop中采用同一个超参数。
		S_dW=β_2*S_dW + (1-β_2)*(dW)^2
		S_db=β_2*S_db + (1-β_2)*(db)^2
	细节2：要确保你的算法不会除以0。
	如果S_dW的平方根趋近于0怎么办？dW/√(S_dW)得到的答案就会非常大，为了确保数值稳定，在实际中操练的时候，需要在分母上加上一个很小
	很小的ε，ε=10^-8是个不错的选择，这只是保证数值能稳定一些。所以RMSprop 和 Momentum有很相似的一点，可以消除梯度下降中的摆动，
	包括mini-batch梯度下降，并允许你使用一个更大的学习率α，从而加快你的算法学习速度.
	
	使用Adam算法，首先你要初始化：
	V_dW=0,	S_dW=0,	V_db=0,	S_db=0
	在第t次迭代中，你要计算微分，用当前的mini-batch计算dW,db（一般你会用mini-batch梯度下降法）。
	接下来计算Momentum指数加权平均数，所以V_dW=β_1*V_dW + (1-β_1)*dW,因为后面RMSprop要用到β_2。
	使用Momentum时，我们肯定会用这个公式。同样 V_db=β_1*V_db + (1-β_1)*db 。
	接着你用RMSprop进行更新，即用不同的超参数β_2：
	S_dW=β_1*S_dW + (1-β_2)*(dW)^2
	S_db=β_1*S_db + (1-β_1)*(db)^2
	相当于Momentum更新了超参数β_1，RMSprop更新了超参数β_2.
	
	一般使用Adam算法的时候，要计算偏差修正：
	修正的V_dW=V_dW/(1-β_1^t),
	修正的V_dW=V_db/(1-β_1^t);t是迭代次数；
	
	同样，
	修正的S_dW=S_dW/(1-β_2^t)，
	修正的S_db=S_db/(1-β_2^t);
	
	最后更新权重：
	W = W - α*(修正后的V_dW)/[√(修正后的S_dW)+ε]
	b = b - α*(修正后的V_db)/[√(修正后的S_db)+ε]
	
	所以Adam算法结合了Momentum和RMSprop梯度下降法，并且是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。
	本算法中有很多的超参数，学习率α很重要，也经常需要调试，你可以尝试一系列值，然后看哪个有效。
	β_1常用的缺省值为0.9，这是dW的移动平均数，也就是dW的加权平均数，这是Momentum涉及的项，至于超参数β_2，Adam论文的作者，也就是Adam
	算法的发明者，推荐使用0.999，这是在计算(dW)^2以及(db)^2的移动加权平均值。关于ε的选择其实没那么重要，Adam论文的作者建议ε为10^-8,
	但你并不需要设置它，因为它并不会影响算法表现。但是在使用Adam的时候，人们往往用缺省值即可，β_1,β_2,ε都是如此。我觉得没人会去调整ε。
	然后尝试不同的α值，看看那个效果最好。你也可以调整β_1,β_2，但我认识的业内人士很少这么干。
	
	为什么这个算法叫Adam？Adam代表的是：adaptive Momentum Estimation：自适应动量估计
	β_1用于计算这个微分dW，叫做第一矩（first moment），β_2用来计算平方数的指数加权平均数(dW)^2，叫做第二矩（second moment）。
	Adam的名字由此而来。
		
【7】学习率衰减
	α=1/(1+decay_rate*epoch_num) * α_0 ; α_0是初始学习率，epoch表示第几次遍历训练集，例如第4次迭代就叫做epoch_4,叫做第4代。
	注意这个decay_rate是另一个你需要调整的超参数。
	例如，你已经迭代了几次，如果 α_0为0.2，衰减率decay_rate为1，那么在
	第1代中，α=1/(1+1*1) * 0.2=0.1
	第2代中，α=1/(1+1*2) * 0.2=0.067
	第3代中，α=1/(1+1*3) * 0.2=0.05
	第4代中，α=1/(1+1*4) * 0.2=0.04
	...
	
	其他的学习率衰减：
	指数衰减：
	α=0.95^epoch_num * α_0
	
	α=k/(√epoch_num) * α_0
	α=k/(√t) * α_0			，t表示mini-batch的number
	还有一种学习率离散下降的方法：即学习率一会下降一半，一会再下降一半。
	有时候人们也会使用手动衰减。如果你一次只训练一个模型，如果你要花上数小时或数天来训练，有的人的确会这么做，看着自己的模型训练，
	耗上数日，然后他们觉得学习率变慢了，我把α调小一点。手动控制α当然有用，一小时复一小时，一日复一日地手动调整α，只有模型数量小的
	时候有用。

【8】局部最优 与 鞍点

	我自己写的：【在低维度空间，你更有可能碰到局部最优】
				【在高维度空间，你更有可能碰到鞍点，而不会碰到局部最优】
				
	通常梯度为0的点，并不是这个图中的局部最优点。实际上成本函数的零梯度点通常是鞍点。一个具有高维空间的函数，如果梯度为0，那么
	在每个方向，它可能是凸函数，也可能是凹函数。【如果你在2万维空间中，那么要想得到局部最优，所有的2万个方向都需要是凸函数这样】，
	但是发生的几率也许很小，也许是2^(-20000),你更有可能遇到有些方向的曲线会这样向上弯曲（凸函数），另一些方向曲线向下弯曲（凹函数），
	而并不是所有的都向上弯曲。因此【在高维度空间，你更有可能碰到鞍点，而不会碰到局部最优】。
	至于为什么把一个平面surface叫做鞍点，可以这么理解：那图像就像是马背部的马鞍一样，你坐在马鞍上，那么这里的导数为0的点叫做鞍点，
	就是你坐在马鞍上的那一点，而这里导数为0。
	
	马鞍图：马前腿部和后腿部高，中间低，即凸函数；左边肚子和右边肚子低，脊柱高，即凹函数。
	我感觉马鞍想象成骆驼的双峰之间也很形象。即在两个方向（这里以两个方向为例解释，高维空间时有很多方向）上分别是凹函数与凸函数。
	
	所以我们从深度学习历史中学到的一课就是我们对【低维度空间】的大部分直觉，比如你可以画出左边的图（局部最优图），但是并不能应用到
	高维度空间中，但是适用于其他算法。如果你有2万个参数，那么J函数由2万个维度向量，你更有可能遇到鞍点，而不是局部最优点。
	如果局部最优不是问题，那么问题是什么？结果是（马鞍图）平稳段会减缓学习，平稳段是一块区域，其中导数长时间接近于0。如果你在马鞍
	的左上部，梯度会从平面从上向下下降，因为梯度等于或接近0，平面很水平，你得花上很长时间，（沿着脊柱）慢慢抵达平稳段的这个点（靠近
	鞍点），因为左边或右边的随机扰动，你的算法能够走出平稳段（即走到鞍点附近后，随机扰动使得算法可以走到鞍点下面去，离开脊柱方向沿着
	肚子往下走）。
	
	这节课程的要点是：首先，你不太可能困在极差的局部最优中，条件是你在训练较大的神经网络，存在大量参数，并且成本函数J被定义在较高的
	维度空间。第二点：平稳段是一个问题，这样使得学习十分缓慢，这也是像Momentum或是RMSprop或者Adam这样的算法能够加速学习算法的地方。
	在这些情况下，更成熟的观察算法如adam算法能够加快速度，让你尽早往下走出平稳段。
	
	
		
		
		
		
		
	