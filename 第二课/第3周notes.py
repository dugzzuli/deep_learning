
# 本周内容：超参数调试、batch正则化和程序框架

【1】调试处理，为超参数选择合适的范围
	【1.0】由粗到细的搜索方法
		方法就是先大致找到某个工作比较好的点，然后在该点附近进行细致搜索。
	【1.1】给学习率α取值
	例如，当α取值从0.00001到1时，使用随机取值的方法，用线性轴取值不是个好方法，可以使用【对数取值方法】：
    0.0001到0.001,   0.001到0.01,      0.01到0.1		  0.1到1
	10^-4到10^-3,	10^-3到10^-2,	  10^-2到10^-1,		10^-1到10^0,
	取对数：
	lg10^-4到1g10^-3， lg10^-3到1g10^-2，lg10^-2到1g10^-1， lg10^-1到1g10^0
	对数距离都为1。
	
	r=-4*np.random.rand(),即r取值[-4,0]，rand()是随机取值为0到1的任意一个值，
										 rand(2,2)是2*2矩阵，每个元素随机取值为0到1的任意一个值
	α=10^r,			        α取值[10^-4,10^0]
	如果你在10^a与10^b之间取值(本例中就是在10^-4到10^0取值),你要做的就是在[a,b]区间(本例中即[-4,0])随机均匀地给r取值。
	在对数坐标上取值，取最小值的对数(即取0.0001的对数，得到-4)就得到a值；取最大值的对数(即取1的对数，得到0)就得到b值。
	所以现在你在对数轴上的10^a到10^b区间取值，在[a,b]随意均匀地选取r值，将超参数设置为10^r，这就是在对数轴上取值的过程。
	
	【1.2】给β取值
		β用于计算指数加权平均值。假设你认为β是0.9到0.999之间的某个值，也许这就是你想搜索的范围。请记住这一点：当计算
		指数加权平均值时，取0.9就像在10个值中计算平均值，有点类似于计算10天的温度平均值；而取0.999就是在1000个值中取平均。
		所以，如果你想在0.9到0.999区间搜索，那就不能用线性轴取值，不要随机均匀在此区间取值。最好的方法就是，我们探究1-β，
		此值在0.1到0.001区间内，所以我们会给1-β取值，[10^-3,10-1],你要做的就是在[-3,-1]里随机均匀地给r取值：
		1-β=10^r，所以β=1-10^r，这就变成了在特定范围内你的超参数随机取值。
		
		为什么用线性轴取值不是个好方法？
		这是因为当β接近1时，所得结果的灵敏度会变化，即使β有微小的变化。如果β在0.9到0.9005之间取值，无关紧要，你的结果几乎
		不会变化，但β值如果在0.999到0.9995之间，这会对你的算法产生巨大影响。在这两种情况下，前者0.9到0.9005都是根据大概十个值
		取平均；但后者，0.999是基于1000个值，0.9995是基于2000个值，因为这个公式1/(1-β)，当β接近1时，β就会对细微的变化
		变得很敏感。所以整个取值过程中，你需要更加密集地取值，在β接近1的区间内，或者说，当1-β接近0时，这样你就可以更加有效
		的分布取样点，更有效的探究可能的结果。
		
【2】正则化网络的激活函数
	【2.1】batch归一化
	batch归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更庞大，工作效果也很好，也会
	使你很容易地训练甚至是深层网络。
	
	当训练一个模型，比如logistic回归时，归一化输入特征可加速学习过程，你计算了平均值μ，从训练集中减去平均值x=x-μ，计算了方
	差σ^2，接着根据方差来归一化你的数据集x=x/σ^2,这对logistic回归和神经网络的归一化输入特征值而言是很有用的。
	
	那么更深的模型呢？你不仅输入了特征值x，而且各层有激活值a^[l]。如果你想训练这些参数，比如w^[3],b^[3],那归一化a^[2]的平均
	值和方差以便使w^[3],b^[3]的训练更有意义岂不是很好？
	logistic回归的例子中，我们看到了如何归一化输入特征会帮助你更有效的训练w和b。所以问题来了。对于任意一个隐藏层而言例如第3层，
	我们能否规一化a^[2]值，以便更快地训练w^[3],b^[3]？简单来说，这就是batch归一化的作用。
	尽管严格来说，我们真正归一化的不是a^[2],而是z^[2]。深度学习中有个争论就是，应该先归一化z再应用于a，还是直接归一化a？
	实践中经常做的是归一化z^[2],而不是归一化a^[2]。
	
	batch归一化的使用方法：
		在神经网络中，已知一些中间值，假设你有一些隐藏单元值，从z^(1)到z^(m)。(注意这里圆括号是某层的元素，不是中括号表示的层数)。
		