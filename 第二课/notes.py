【0】总结一下，在机器学习中，我们通常将样本分成训练集、验证集和测试集三部分。数据集规模相对较小的，适用传统的划分比例60%,20%,20%；
	数据规模较大的，验证集和测试集可以占到数据总量的20%或10%以下.例如100万条数据，1万条作验证集，1万条作测试集，训练集占98%。
	验证集是用来评估不同的模型；测试集是对最终所选定的神经网络系统做出【无偏评估】。如果不需要无偏评估，也可以不设置测试集。
	如果只有验证集，没有测试集，我们要做的是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。
	因为验证集中已经涵盖测试集数据，其不再提供无偏性能估计。
	在机器学习中，如果只有一个训练集和一个验证集，而没有独立的测试集，则验证集被称为测试集。不过在实际应用中，人们只是把测试集当成
	简单交叉验证集使用，并没有完全实现该术语的功能，因为他们把验证集数据过度拟合到了测试集中。如果某团队跟你说他们只设置了一个训练集
	和一个测试集，我会很谨慎，心想他们是不是真的有训练验证集，因为他们把验证集数据过度拟合到了测试集中，让这些团队改变叫法，
	改称其为"训练验证集"，而不是"训练测试集"可能不太容易，即便我认为在专业用词上更准确。实际上，如果你不需要无偏评估算法性能，
	那么这样是ok的。
	
【1】深度学习的另一个趋势是，越来越多的人在训练集和测试集分布不匹配的请情况下进行训练，假设你要构建一个用户可以上传大量图片的应用程序，
	目的是找出并呈现所有的猫咪图片，训练集可能是从网上下载的猫咪图片，而验证集和测试集是用户在这个应用上上传的猫咪的图片。
	就是说，训练集可能是从网上抓下来的图片，分辨率很高，而验证集和测试集使用户上传的图片（可能是手机随意拍摄的），分辨率很低，比较模糊。
	这两类数据有所不同。根据经验，我建议大家 要确保验证集和测试集的数据来自同一分布。
	
【2】偏差和方差   bias/variance
	偏差高，不能很好的拟合数据集，欠拟合    举的例子图是二分类logistic
	分类器方差较高，数据过度拟合，过拟合    举的例子图是神经网络过拟合分类的图
	
	训练集误差和验证集误差  train set error  /   dev set error
	(1)训练集误差(1%)低，验证集误差(11%)高，则称之为高方差
	(2)训练集数据的拟合度不高，就是数据欠拟合，就可以说这种算法偏差比较高；
		相反，他对于验证集产生的结果却是合理的，验证集中的错误率(16%)只比训练集的错误率(15)多1%，所以这种算法偏差高.
	(3)训练集错误率是15%，偏差相当高，但是验证集的评估结果更糟糕，错误率达到30%。
		这种情况下，我会认为这种算法偏差高，因为它在训练集上结果不理想，方差也很高，这是方差偏差都很高的情况。
	(4)训练集错误率为0.5%，验证集错误率为1%，则方差和偏差都很低。
			
	一般来说，最优误差也被称为贝叶斯误差
	# 如果最优误差或贝叶斯误差非常高，比如15%，15%的错误率对训练集来说是非常合理的，偏差不高，方差也非常低。【感觉这句话有问题，我没理解】

	先检查偏差，再检查方差：
	(1)如果偏差高，(欠拟合)，就试一下:1.选择一个新网络(比如含有更多隐藏层或者隐藏单元的网络)。2.花费更多的时间来训练网络，
		或者尝试更先进的优化算法。    不过采用规模更大的网络通常会有所帮助，延长训练时间不一定有用，但也没什么坏处。
		训练学习算法时，需要不断尝试这些方法，直到解决掉偏差问题，这是最低标准，反复尝试，直到可以拟合数据为止。
		至少能够拟合训练集。如果网络足够大，通常可以很好的拟合训练集。
	(2)当偏差降低到可接受的数值，检查一下方差有没有问题，为了评估方差，我们要查看验证集性能；
		如果方差高，最好的解决方法就是【采用更多数据】。但有时候，我们无法获得更多数据，我们也可以尝试通过【正则化】来减少过拟合。
	(3)反复各种尝试，直到找到一个低偏差，低方差的框架，这样你就成功了。
	
	有两点需要注意：
	(1)高偏差和高方差是两种不同的情况，我们后续要尝试的方法也可能完全不同。我们通常会用训练验证集来诊断算法是否存在偏差
		或方差问题。，然后根据结果选择尝试部分方法。
		举个例子，如果算法存在高偏差问题，准备更多训练数据其实什么用都没有。所以大家要清楚到底是高方差还是高偏差问题。
		明确这一点有助于我们选出最有效的方法。
	(2)在机器学习的初期阶段，关于偏差方差权衡的探讨屡见不鲜。原因是我们可以尝试的方法有很多，可以增加偏差，减少方差。
		但是在深度学习的早期阶段，我们没有太多工具可以做到只减少偏差或方差，却不影响到另一个。但是在当前的深度学习和大数据时代，
		只要持续训练一个更大的网络，只要准备了更多数据，那么也并非只有这两种情况。
		只要正则适度，通常构建一个更大的网络便可以在不影响方差的同时，减少偏差。而采用更多数据通常可以在不过多影响偏差的同时减少方差。
		这两步实际要做的工作是训练网络、选择网络或者准备更多数据，现在我们有工具可以做到在只减少偏差的同时，不对另一方产生过多不良影响。
		我觉得这就是深度学习对监督式学习大有裨益的一个重要原因。也是我们不用太过关注如何平衡偏差和方差的一个重要原因。
		
【3】正则化-------------------第二课--第一周--1.4-正则化
	【3.1】以logistic回归为例：(与神经网络的区别是logistic回归相当于只有一层的神经网络)
		【L1正则化】：R(w)=λ/m*|w|，    其中|w|是1范数
		【L2正则化】：R(w)=λ/2m*||w||^2    其中||w||是二范数，并且||w||^2=w^T*w
		
		代价函数：
			J(w,b)=1/m *ΣL(a(i),y)
			J(w,b)=-1/m *Σ[y(i)log(a(i))+(1-y(i))log(1-a(i))]
		
		加入L2正则化的代价函数：
			J(w,b)=-1/m *Σ[y(i)log(a(i))+(1-y(i))log(1-a(i))] + λ/2m*||w||^2
	
	【3.2】以神经网络为例：
		在神经网络中实现带L2正则化的过程：
		L2正则化:R(w) = λ/2m*Σ||w^[l]||F^2
			正则化项中的Σ求l=1到l=L层。w^[l]表示第l层的所有权重.每层的权重组成一个矩阵。
			||w^[l]||F^2 被定义为矩阵中所有元素的平方求和。F是下标，指代弗罗贝尼乌斯范数，表示求矩阵中所有元素的平方和。
			
		成本函数：J(w,b)=1/m *ΣL(a(i),y) + λ/2m*Σ||w^[l]||F^2，	
		成本函数中包含从w[1],b[1]到w[L],b[l]共L层的所有参数
			注意这里的||w[l]||F^2是神经网络的所有层的矩阵L2范数,称为Frobenius弗罗贝尼乌斯范数，
			||w^[l]||F^2是(l-1)层到l层的各个权重的平方之和ΣΣ(wij)^2，这里两个求和符号Σ分别遍历(l-1)的第i个神经元，另一个表示遍历第l层的第j个神经元。
			Σ||w[l]||F^2 = 所有层的各个权重的平方之和ΣΣΣ(wij)^2
			
	【3.3】如何使用弗罗贝尼乌斯范数实现【梯度下降】呢？
		答：用反向传播计算出dw的值，反向传播会给出∂J/∂w^[l]
		每层的梯度为：
			  dw^[l]=(...) + λ/m*w^[l], 其中(...)是正则化前面的项对w^[l]求导，不方便用式子表达出来。
										λ/m*w^[l]是正则化项R(w) = λ/2m*Σ||w^[l]||F^2对w^[l]求导得到的
		更新：w^[l] = w^[l]-α*dw^[l]
					= w^[l]-α*λ/m*w^[l]-α*(...)     #即与不加入L2正则化的区别是多了-α*λ/m*w^[l]
				#   =(1-α*λ/m)*w^[l]-α*(...)}   L2正则化也被称为'权重衰减',即权重指标乘以了一个小于1的系数，所以叫权重衰减。
	
	【3.4】为什么只正则化参数w?为什么不再加上参数b呢？
		答：因为w通常是一个高维参数矢量，已经可以表达高偏差问题，w可能含有很多参数，我们不可能拟合所有参数，而b只是单个数字。
		如果加上b，其实也没多大影响，因为b只是众多参数中的一个，所以通常省略b就行。
	
	【3.5】【L1正则化】与【L2正则化】的区别：
		(1)L1正则化会让参数w变得稀疏。所谓参数变得更稀疏是指会有更多的参数变为0，这样可以达到类似特征提取的功能。
		   L2正则化不会让参数w变得更稀疏的原因是当参数很小时(如0.001)，这个参数的平方基本商就可以忽略了，于是模型
		   不会进一步将这个参数调整为0。
		(2)L1正则化的计算公式不可导，而L2正则化公式可导。因为在优化参数时需要计算损失函数的偏导数，所以对
		   含有L2正则化损失函数的优化要更简洁，优化带L1正则化的损失函数要更加复杂，而且优化方法有很多种，
		   在实践中也可以将L1正则化和L2正则化同时使用: R(w)=Σ {α|w|+(1-α)w^2}					
	
	【3.6】为什么正则化有利于预防过拟合呢？为什么他可以减少方差问题？
		答：加入正则化后，把λ设置足够大，w^[l]设置足够小接近于0.直观理解就是把许多隐藏单元的权重设为0，于是基本消除了这些隐藏单元
		的许多影响，如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会
		使这个网络从过拟合状态更接近于高偏差状态，但是λ会存在一个中间值，即处于高方差和高偏差状态的正中间的'just right'的中间状态。
		直观理解就是λ增加到足够大，w会设置为接近于0。实际上不会发生这种情况的，我们尝试消除或者至少减少许多隐藏单元的影响，最终这个
		网络会变得更简单，这个神经网络越来越接近于逻辑回归。
		我们直觉上会认为大量隐藏单元被完全消除了，其实不然，实际上该神经网络的所有隐藏单元依然存在，但是他们的影响变得更小了，神经
		网络变得更简单了，貌似这样更不容易发生过拟合，吴恩达不确定这个直觉经验是否有用，不过在编程中执行正则化时，你会实际看到一些
		方差减少的结果。
		
		我的理解：过拟合的情况就是得到了极复杂的高度非线性函数。防止过拟合就是使得到的函数不那么复杂，使其变成线性函数或者简单的曲线就行。
		

		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		

		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
	
		