机器学习的应用是一个高度依赖经验的过程，伴随着大量迭代过程，你需要训练诸多模型，才能找到合适的那一个，所以，优化算法能够
帮助你快速训练模型，其中一个难点在于，深度学习没有在大数据领域发挥最大的效果，我们可以利用一个巨大的数据集来训练神经网络，
而在巨大数据及基础上进行训练速度很慢，因此，使用快速的好用的优化算法能够大大提高你和团队的效率。
1.【mini-batch梯度下降法】
	向量化能让你有效地对所有m个例子进行计算，允许你处理整个训练集，而无需某个明确的公式。所以我们要把训练样本放到巨大的矩阵X当中去。
	X=[x^(1) x^(2) x^(3) ... x^(m)],m个样本,X的维数是(n_x,m)
	Y=[y^(1) y^(2) y^(3) ... y^(m)],Y的维数是(1,m)
	但是如果样本总数m很大的话，处理速度仍然缓慢，比如说，如果m是500万或者更大的数，在对整个训练集执行梯度下降法时，你必须处理整个
	训练集，然后才能进行一步梯度下降法；然后你需要再重新处理500万个训练样本，才能进行下一步梯度下降。所以如果你在处理完整个500万
	个样本的训练集之前，先让梯度下降法处理一部分，你的算法速度会更快。
	
	你可以把训练集分割为小一点的子训练集，这些子训练集被取名为Mini-batch,假设每一个子集中只有1000个样本，那么把其中的x^(1)到x^(1000)
	取出来，将其称为第一个子训练集，也叫作mini-batch;然后你再取出接下来的1000个样本，从x^(1001)到x^(2000)，然后再取1000个样本；以此类推
	将x^(1)到x^(1000)的样本称为X^{1},x^(1001)到x^(2000)的样本称为X^{2},...,最后是X^{5000}，即总共有5000个mini-batch。
	对Y也要进行相同处理，相应的拆分Y的训练集，y^(1)到y^(1000)的样本称为Y^{1},y^(1001)到y^(2000)的样本称为Y^{2},...,最后是Y^{5000}。
	mini-batch的数量t组成了X^{t}和Y^{t}
	X的维数是(n_x,m)，X^{t}的维数应该是(n_x,1000),Y^{t}的维数都是(1,1000).
	batch梯度下降法指的是我们之前讲过的梯度下降法，就是同时处理整个训练集。
	mini-batch梯度下降法指的是每次同时处理单个mini-batch的X^{t}和Y^{t}。
	
	while 没达到要求 ：
		for t in range(1,5000):		#注意：Z[1],A[1],...,Z[L],A[L]都是向量。维度为(1,1000)?
		   forwardprop on X^{t}
			Z[1]=W[1]X^{t}+b[1]
			A[1]=g[1](Z[1])
			...
			Z[L]=W[L]X^{t}+b[L]
			A[L]=g[L](Z[L])    		#A[L]是预测值
			
			compute cost J^{t}=1/1000*ΣL(a(i),y(i)) + λ/2m*Σ||w^[l]||F^2
			
			backprop to compute gradients (执行反向传播来计算J^{t}的梯度)：
			W[l]=W[l]-α*dW[l]
			b[l]=b[l]-α*db[l]
	
	如果mini-batch size=m,就是batch梯度下降
	如果mini-batch size=1,就是随机梯度下降
	
	随机梯度下降，每次迭代，你只对一个样本进行梯度下降，大部分时候你向着全局最小值靠近，有时候你会远离最小值，因为那个样本恰好给你
	指的方向不对，因此随机梯度下降是有很多噪声的。平均看来，它最终会靠近最小值，不过有时候也会方向错误，因为随机梯度下降法永远不会
	收敛，而是会一直在最小值附近波动，但它并不会达到最小值并停留在此。
	 实际上你选择的mini-batch size大小在1和m之间,而1太小，m太大了。原因在于使用batch梯度下降法，每次迭代需要处理大量训练样本，该算法
	 的主要弊端在于特别是训练样本数量巨大的时候，单词迭代耗时太长，如果驯良样本不大，batch梯度下降法也能运行的很好。相反，如果使用
	 随机梯度下降法，如果你只要处理一个样本，那这个方法很好，这样做没有问题，通过减小学习率，噪声会被改善或有所减少，但随机梯度下降
	 法的一大缺点是，你会失去所有向量化带给你的加速。因为一次性只处理了一个训练样本，这样效率过于低下。所以实践中最好选择不大不小的
	 mini-batch尺寸，实际上学习率达到最快，你会发现两个好处，一方面你得到了大量向量化，另一方面，你不需要等待整个训练集被处理完，就
	 可以进行后续工作。每代(epoch)训练集允许我们采取5000个梯度下降步骤。
	 怎样选择mini-batch的尺寸？
	 如果样本集少于2000个样本，就直接用batch梯度下降法
	 如果样本数量较大的话，就使用mini-batch梯度下降法。一般的mini-batch尺寸大小为64到512。考虑到电脑内存设置和使用的方式，如果
	 mini-batch大小是2的某次方的时候(64、128、256、512)，代码会运行的额更快。
	 X^{t}和Y^{t}要符合cpu/gpu内存，如果你处理的mini-batch和cpu/gpu内存不相符，不管你用什么方法处理数据，你会注意到算法的表现急转直下，
	 变得惨不忍睹。
	 一般需要多试几次mini-batch尺寸的值，
	
	
		
		
		
		
		
		
	