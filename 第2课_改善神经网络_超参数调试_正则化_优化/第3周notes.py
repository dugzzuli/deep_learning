
# 本周内容：超参数调试、batch正则化和程序框架

【1】调试处理，为超参数选择合适的范围
	【1.0】由粗到细的搜索方法
		方法就是先大致找到某个工作比较好的点，然后在该点附近进行细致搜索。
	【1.1】给学习率α取值
	例如，当α取值从0.00001到1时，使用随机取值的方法，用线性轴取值不是个好方法，可以使用【对数取值方法】：
    0.0001到0.001,   0.001到0.01,      0.01到0.1		  0.1到1
	10^-4到10^-3,	10^-3到10^-2,	  10^-2到10^-1,		10^-1到10^0,
	取对数：
	lg10^-4到1g10^-3， lg10^-3到1g10^-2，lg10^-2到1g10^-1， lg10^-1到1g10^0
	对数距离都为1。
	
	r=-4*np.random.rand(),即r取值[-4,0]，rand()是随机取值为0到1的任意一个值，
										 rand(2,2)是2*2矩阵，每个元素随机取值为0到1的任意一个值
	α=10^r,			        α取值[10^-4,10^0]
	如果你在10^a与10^b之间取值(本例中就是在10^-4到10^0取值),你要做的就是在[a,b]区间(本例中即[-4,0])随机均匀地给r取值。
	在对数坐标上取值，取最小值的对数(即取0.0001的对数，得到-4)就得到a值；取最大值的对数(即取1的对数，得到0)就得到b值。
	所以现在你在对数轴上的10^a到10^b区间取值，在[a,b]随意均匀地选取r值，将超参数设置为10^r，这就是在对数轴上取值的过程。
	
	【1.2】给β取值
		β用于计算指数加权平均值。假设你认为β是0.9到0.999之间的某个值，也许这就是你想搜索的范围。请记住这一点：当计算
		指数加权平均值时，取0.9就像在10个值中计算平均值，有点类似于计算10天的温度平均值；而取0.999就是在1000个值中取平均。
		所以，如果你想在0.9到0.999区间搜索，那就不能用线性轴取值，不要随机均匀在此区间取值。最好的方法就是，我们探究1-β，
		此值在0.1到0.001区间内，所以我们会给1-β取值，[10^-3,10-1],你要做的就是在[-3,-1]里随机均匀地给r取值：
		1-β=10^r，所以β=1-10^r，这就变成了在特定范围内你的超参数随机取值。
		
		为什么用线性轴取值不是个好方法？
		这是因为当β接近1时，所得结果的灵敏度会变化，即使β有微小的变化。如果β在0.9到0.9005之间取值，无关紧要，你的结果几乎
		不会变化，但β值如果在0.999到0.9995之间，这会对你的算法产生巨大影响。在这两种情况下，前者0.9到0.9005都是根据大概十个值
		取平均；但后者，0.999是基于1000个值，0.9995是基于2000个值，因为这个公式1/(1-β)，当β接近1时，β就会对细微的变化
		变得很敏感。所以整个取值过程中，你需要更加密集地取值，在β接近1的区间内，或者说，当1-β接近0时，这样你就可以更加有效
		的分布取样点，更有效的探究可能的结果。
		
【2】正则化网络的激活函数
	【2.1】batch归一化
	batch归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更庞大，工作效果也很好，也会
	使你很容易地训练甚至是深层网络。
	
	当训练一个模型，比如logistic回归时，归一化输入特征可加速学习过程，你计算了平均值μ，从训练集中减去平均值x=x-μ，计算了方
	差σ^2=1/m*∑(x-μ)^2，接着根据方差来归一化你的数据集x=x/σ^2,这对logistic回归和神经网络的归一化输入特征值而言是很有用的。
	
	那么更深的模型呢？你不仅输入了特征值x，而且各层有激活值a^[l]。如果你想训练这些参数，比如w^[3],b^[3],那归一化a^[2]的平均
	值和方差以便使w^[3],b^[3]的训练更有意义岂不是很好？
	logistic回归的例子中，我们看到了如何归一化输入特征会帮助你更有效的训练w和b。所以问题来了。对于任意一个隐藏层而言例如第3层，
	我们能否规一化a^[2]值，以便更快地训练w^[3],b^[3]？简单来说，这就是batch归一化的作用。
	尽管严格来说，我们真正归一化的不是a^[2],而是z^[2]。深度学习中有个争论就是，应该先归一化z再应用于a，还是直接归一化a？
	实践中经常做的是归一化z^[2],而不是归一化a^[2]。
	
	batch归一化的使用方法：
	在神经网络中，已知一些中间值，假设你有一些隐藏单元值，从z^(1)到z^(m)。(注意这里圆括号是某层的元素，不是中括号表示的层数,
	这里省略了层数，以便简化符号)。  
	你要计算该层的z平均值：μ=1/m*∑z^(i)；						..........(1)
				 计算方差:σ^2=1/m*∑(z^(i)-μ)^2  				..........(2)
				 最后归一化值为:
						z(norm)^(i)=[z^(i)-μ]/√(σ^2+ε),			..........(3)
						加上ε是为了使数值稳定以防σ=0的情况。
	所以现在，我们已经把这些z值标准化为含平均值0和标准单位方差，所以z的每一个分量都含有平均值0和方差1，但是我们不想让隐藏单元
	总是含有平均值0和方差1，也许隐藏单元有了不同的分布会有意义，
	所以我们要做的就是计算:
						z~^(i)=γ*z(norm)^(i)+β,					..........(4)
						这里γ和β是模型的学习参数，β与Momentum的超参数β没有关系。
	所以我们使用梯度下降或其他一些类似梯度下降的算法，比如Momentum或者Nesterov,Adam，你会更新γ和β，正如更新神经网络
	的权重一样。请注意，γ和β的作用是，你可以随意设置z~的平均值,
	事实上，如果γ=√(σ^2+ε),即如果γ等于z(norm)^(i)=[z^(i)-μ]/√(σ^2+ε)的分母项，β=μ，那γ*z(norm)^(i)+β的作用在于，它会精确转化
	这个方程，如果这些成立，那么z~^(i)=z^(i)。通过对γ和β的合理设定，规范化过程，即这四个等式： 
				μ  = 1/m*∑z^(i)
				σ^2= 1/m*∑(z^(i)-μ)^2
		z(norm)^(i)=[z^(i)-μ]/√(σ^2+ε)
			 z~^(i)=γ*z(norm)^(i)+β
	从根本上来说，只是计算恒等函数，通过赋予γ和β其他值，可以使你构造含其他平均值和方差的隐藏单元值。所以，在网络中匹配这个单元
	的方式是，之前可能是用z^(1),z^(2)等等，现在则会用z~^(i)取代z^(i)，方便神经网络中的后续计算。
	
	batch归一化的作用是，它使用的归一化过程不只是输入层，甚至同样适用于神经网络中的深度隐藏层，你应用归一化了一些隐藏单元中的
	平均值和方差，不过，训练输入和这些隐藏单元值的一个区别是，你也许不想隐藏单元值必须是平均值0和方差1.比如，如果你有sigmoid
	激活函数，你不想让你的值总是全部集中在靠近0的范围，你想是他们有更大的方差或者不是0均值，以便更好的利用非线性的sigmoid函数，
	而不是使所有的值都集中于这个线性版本中，这就是为什么有了γ和β两个参数后，你可以确保所有的z^(i)值可以是你想赋予的任意值，
	或者它的作用是保证隐藏单元以使均值和方差标准化，那里，均值和方差由两参数控制即γ和β。学习算法可以设置为任何值。所以他的真
	正作用是，使隐藏单元值的均值和方差标准化，即z^[i]有固定的均值和方差，均值和方差可以是0和1，也可以是其他值，它是由γ和β两
	参数控制的。
	
	实践中，batch归一化通常和训练集的mini-batch一起使用。你应用batch归一化的方式就是,你用第一个mini-batch应用w^[1]、b^[1]计算
	z^[1]，在其上计算z^[1]的均值和方差，接着batch归一化会减去均值，除以标准差,由β^[1]、γ^[1]重新缩放，这样就得到了z~^[1]；然后
	再得到a^[1],然后用w^[2]、b^[2]计算z^[2]等等....,这么做都是为了在第一个mini-batch上进行一步梯度下降法;
	接着继续第二个mini-batch，等等。
	
	细节：先前，每层的参数是w^[l],b^[l]，还有β^[l]、γ^[l]，请注意计算z的方式如下：z^[l]=w^[l]*a^[l-1]+b^[l],但是batch归一化
	做的是，它要看这个mini-batch，先将z^[l]归一化，结果为均值0和标准方差，再由β和γ重新缩放，但这意味着无论b^[l]的值是多少，
	都是要被减去的，因为在batch归一化的过程中，你要计算z^[l]的均值，再减去平均值，在此例的mini-batch中增加任何常数，数值都
	不会改变，因为加上的任何常数都将会被均值减法所抵消。所以，如果你在使用batch归一化，其实你可以消除这个参数b^[l]，或者你
	也可以暂时把它设置为0。那么参数化变成z^[l]=w^[l]*a^[l-1],然后你计算归一化的z(norm)^[l],z~=γ*z^[l]+β^[l],你最后会用参数
	β^[l]以便决定z~^[l]的取值。
	
	总结一下，因为batch归一化0超过了此层z^[l]的均值，b^[l]这个参数没有意义，所以你必须去掉它，由β^[l]替代，这是个控制参数，
	会影响转移或偏置条件。最后，请记住z^[l]的维数，因为在这个例子中，维数会是(n^[l],1),b^[l]的尺寸是(n^[l],1),那么β^[l]、γ^[l]
	的维度也是(n^[l],1)，因为这是你有的隐藏层的数量，你有n^[l]个隐藏单元，所以β^[l]和γ^[l]用来将每个隐藏层的均值和方差缩放
	为网络想要的值 。
	
	总结一下，如何用batch归一化来应用梯度下降法，假设你在使用mini-batch梯度下降法，你运行同t=1到batch数量的for循环，你会应
	用正向prop于mini-batch，每个隐层都会应用正向prop，用batch归一化替代z^[l]为z~^[l],接下来，它确保在这个mini-batch中，z值
	有归一化的均值和方差，归一化均值和方差是z~^[l]。然后你用反向prop计算dw^[l],db^[l],,及每个l层的所有值dβ^[l]和dγ^[l]。
	尽管严格来说，因为你要去掉b，db^[l]部分其实已经去掉了，最后你更新这些参数，
		w^[l]=w^[l]-*dw^[l],
		β^[l]=β^[l]-*dβ^[l]
		γ^[l]=γ^[l]-*dγ^[l]
	如果你已经将梯度计算成这样了，你就可以使用梯度下降法了。这也适用于有Momentum,RMSprop,Adam的梯度下降法，与其使用梯度下降法
	更新mini-batch,你可以用这些其他的算法来更新。
	
	batch归一化可以起作用的第二个原因是，它可以使权重，比你的网络更滞后或更深层，比如，相比于神经网络中前层比如第一层的权重，
	第10层的权重更能经受的住变化。
	
	
	batch norm 为什么奏效？
	【使得你的数据改变分布】的这个想法，有个有点怪的名字【covariate shift】，想法是这样的：如果你已经学习了从x到y的映射，如果
	x的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数，由x到y映射保持不变，正如此例中，因为
	真实函数是 此图片是否是一只猫，训练你的函数需要变得更加迫切。如果真实函数也改变，情况就更糟了。covariate shift的问题怎么
	应用于神经网络呢？
	
	batch归一化做的是，它减少了这些隐藏单元值分布变化的数量，如果是绘制这些隐藏单元值的分布，也许这是重整值z，这其实是z^[2]_1,
	z^[2]_2,我要绘制两个值而不是四个值，以便我们设想为2D。batch归一化讲的是，z^[2]_1,z^[2]_2的值可以改变，他们的确会改变，当
	神经网络更新参数，在之前层中，batch归一化可确保无论其怎么变化，z^[2]_1,z^[2]_2的均值和方差均保持不变。所以即使z^[2]_1,
	z^[2]_2的值改变，至少他们的均值和方差也会是均值0，方差1，或者不一定必须是均值0，方差1，而是由β^[2]和γ^[2]决定的值。如果
	神经网络选择的话，可强制其为均值0，方差1，或其他任何均值和方差。但它做的是，它限制了在前层的参数更新，会影响数值分布的
	程度，第三层看到这种情况，因此得学习。
	
	batch归一化减少了输入值改变的问题，它的确使这些值变得更稳定，神经网络的之后层就会
	有更坚实的基础，即使输入分布改变了一些，它会改变得更少，它做的是，当前层保持学习，当层改变时，迫使后层适应的程度减小了，
	或者你可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有
	助于加速整个网络的学习，所以，希望这能带给你更好的直觉。重点是batch归一化的意思是，尤其从神经网络后层之一的角度而言，前层
	不会左右移动的那么多，因为它们被同样的均值和方差所限制，所以，这会使得后层的学习工作变得更容易些。
	
	batch归一化还有一个作用，他有轻微的正则化效果。batch归一化中非直观的一件事是每个mini-batch的值为z^[t],z^[l]，在mini-batch
	计算的由均值和方差缩放的，因为在mini-batch上计算的均值和方差，而不是在整个数据集上，均值和方差有一些小噪音，因为它只在你的
	mini-batch上计算，比如64、128或256,或更大的训练例子，因为均值和方差有一点小噪音，因为它只是有一小部分数据估计得出的，缩放
	过程从z^[1]到z~^[l]，过程也有一些噪音，因为他是用有些噪音的均值和方差计算得出的，所以和dropout相似，它往每个隐藏层的激活
	值上增加了噪音，dropout有噪音的方式，它使一个隐藏单元以一定的概率乘以0，以一定的概率乘以1，所以你的dropout含有几重噪音，因
	为它乘以0或1，对比而言，batch归一化含几重噪音，因为标准偏差的缩放和减去均值带来的额外噪音，这里均值和标准偏差的估计值也是
	有噪音的，所以类似于dropout，batch归一化有轻微的正则化效果，因为给隐藏单元添加了噪音，这迫使后部单元不过分依赖任何一个隐藏
	单元，类似于dropout，它给隐藏层增加了噪音，因此有轻微的正则化效果。因为添加的噪音很微小，所以并不是巨大的正则化效果，你可
	以将batch归一化和dropout一起使用，你可以将两者共同使用，如果你想得到dropout更强大的正则化效果。也许另一个非直观的效果是，
	如果你应用了较大的mini-batch，比如说你用了512，而不是64，通过应用较大的mini-batch，你减少了噪音，因此减少了正则化效果，
	这是dropout的一个奇怪性质，就是应用较大的mini-batch，可以减少正则化效果。说到这，我会把batch归一化当成一个规则，这确实
	不是其目的，但有时它会对你的算法有额外的期望效应或非期望效应，但是不要把batch归一化当做规则，把它当做将你归一化隐藏单元
	激活值并加速的方式，我认为正规化几乎是一个意想不到的副作用。
	
	细节：batch归一化一次只能处理一个mini-batch数据，它在mini-batch上计算均值和方差。
	
	总结一下，在训练时，μ和σ^2是在整个mini-batch上计算出来的，包含了像是64或128或其他一定数量的样本；但在测试时，你可能需要
	逐一处理样本。方法是根据你的训练集估算μ和σ^2，估算的方式有很多种，理论上你可以在最终的网络中运行整个训练集来得到μ和σ^2，
	但在实际操作中，我们通常运用指数加权平均来追踪在训练过程中你看到的μ和σ^2的值，还可以用指数加权平均，有时也叫作流动平均，
	来粗略估算μ和σ^2，然后用测试中μ和σ^2的值，来进行你所需的隐藏单元z值得调整。在实践中，不管你用什么方式估算μ和σ^2，这套
	过程都是比较稳健的，因此我不太担心你具体的操作方式。而且如果你是用某种深度学习框架，通常会有默认的估算μ和σ^2的方式，应
	该一样会起到比较好的效果。但在实践中，任何合理的估算你的隐藏单元z值的均值和方差的方式，在测试中应该都会有效。
	
	
【3】softmax回归
	有一种logistic回归的一般形式，叫做softmax回归，能让你在试图识别某一分类时做出预测，或者说是多种分类中的一个，不只是
	识别两个分类。
	需要分成几类，输出层就有几个神经元，最终的输出y^就是几维向量，每个神经元输出的是属于该类的概率P(ci|x),并且所有输出
	神经元的几个数字相加等于1。让你的网络做到这一点的标准模型要用到softmax层，以及输出层来生成输出。
	
	在神经网络的最后一层，你将会像往常一样计算各层的线性部分z^[L],这是最后一层（L层）的z变量，计算方法是
	z^[L]=W^[L]*a^[L-1]+b^[L],算出了Z^[L],你需要应用softmax激活函数。首先我们要计算一个临时变量t,
		t=e^(z^[L]),即e的z^[L]
	次方，这适用于每一个元素，这是对所有元素求幂，然后输出的a^[L]，基本上就是向量t，但是会【归一化】，使和为1，即
		a^[L]=e^(z^[L])/∑t_i
		a^[L]_i=t_i/∑t_i
	看个例子，假设你算出了z^[L]，z^[L]是一个四维向量，假设为[5 2 -1 3].T,是列向量。我们要做的就是用这个元素取幂方法来计算t。
	因此t=[e^5 e^2 e^-1 e^3].T=[148.4 7.4 0.4 20.1].T,我们从向量t得到向量a^[L]就只需将这些项目【归一化】使总和为1，如果你把t
	的元素都加起来，得到∑t_i=176.3，最终a^[L]=t/176.3,因此，输出层的第一个神经元输出e^5/176.3=0.842=84.2%。同理得到另外三个
	输出神经元的输出e^2/176.3=0.042=4.2%，e^-1/176.3=0.002=0.2%,e^3/176.3=0.114=11.4%。即该输入图片分别属于类别0、1、2、3的
	概率分别是84.2%，4.2%，0.2%，11.4%。神经网络的输出a^[L],也就是y^，是一个4*1维向量，这个4*1维向量的元素就是我们算出来的
	这四个数字。这种算法通过向量z^[L]计算出总和为1的四个概率。
	
	softmax分类器还可以代表其他的什么东西呢？
	举个例子，你有两个输入x1,x2,他们直接输入到softmax层，他有三四个或者更多的输出节点，输出y^。我将向你展示一个没有隐藏层的
	神经网络，他所作的就是计算z^[1]=W^[1]*x+b^[1],而输出的a^[1]或y^=softmax_function(z^[1]).这个没有隐藏层的神经网络应该能
	让你对softmax函数能够代表的东西有所了解。
	两个输入x1,x2，3个输出分类的softmax层能够代表这种类型的决策边界，没有隐藏层的神经网络的决策边界都是线性的。有隐藏层的神
	经网络可以训练出非线性决策边界。
	