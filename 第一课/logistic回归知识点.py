# logistic回归--------(摘自吴恩达深度学习视频)
# logistic回归与神经网络的关系：logistic回归相当于只有输入层没有隐藏层，只有一个输出节点的神经网络
# （这是吴恩达非线性激活函数那一节讲到的，时间03:00即三分0秒时刻）。和感知机一样？
	【0】logistic回归是二类分类问题，对应标签y为0和1。
		激活函数是sigmoid函数, a=σ(z)
		假设只有每个样本只有两个特征x1,x2：
			z=w(T)x + b = w1*x1+w2*x2+b,  
	【编程时有种约定】da表示dL(a,y)/da ;dz表示dL(a,y)/dz     也就是说，把dL/省略了

	【1】单个样本损失函数：L(a,y)=-[ylog(a)+(1-y)log(1-a)] ,#【这个公式很重要，记清楚】
		其中a是预测值、激活值,a属于0到1之间的值,也就是y帽,y是标签。      
		
	【1.0】L(a,y)对a求导得：	
			dL(a,y)/da = -y/a + (1-y)/(1-a);#【这个公式很重要，记清楚】  

			dL(a,y)/dz = a-y;     #【这个公式很重要，记清楚】  
			# 证明：
			# 因为dL/dz = (dL/da)(da/dz),	
			# 又公式σ'(z)=σ(z)*[1-σ(z)]		#【这个公式很重要，记清楚,橙色深度学习上有推导】
			# 即	   a’=a*(1-a) 	,其中a=σ(z)
			# 所以dL(a,y)/dz = a-y   
			
			dL/dw1 = (dL/dz)(dz/dw1) =x1*(dL/dz);
			dL/dw2 = x2*(dL/dz);(同理)
			dL/db  = dL/dz       
			
		编程中会把上面的式子这么表示：
		dw1 = dL/dw1 = x1*dz;
		dw2 = dL/dw2 = x2*dz;
		 db = dL/db  = dz	【这三行中的dz表示dL(a,y)/dz】
		
		【重点来了】求出了上面的这些式子，就可以求出单个样本的一次梯度迭代(更新)了：
		w1 = w1-α*dw1			即[w1 = w1-α*x1*(a-y)]
		w2 = w2-α*dw2			即[w2 = w2-α*x2*(a-y)]
		 b = b -α*db			即[ b = b -α*(a-y)]
		   
	【1.1】当标签y为0时,损失函数为-log(1-a),为使损失函数越小,就必须使a无限接近0。
			即标签为0时，让损失函数越小会使得其预测值(激活值)会趋向于等于0，正是趋向于正确答案的操作。           
	【1.2】当标签y为1时,损失函数为-log(a)  ,为使损失函数越小,就必须使a无限接近1。
			即标签为1时，让损失函数越小会使得其预测值(激活值)会趋向于等于1，正是趋向于正确答案的操作。
			
	【2】总体成本函数：
			J(w,b)=-1/m *Σ[y(i)log(a(i))+(1-y(i))log(1-a(i))],即对m个样本求和后再平均，Σ是求和符号  
			也即J(w,b)=-1/m *ΣL(a(i),y)		其中a=σ(z)，a(i)=σ(z(i))是第i个样本的激活值(预测值)
			
	【3.1】损失函数：是衡量单一训练样本的效果      
	【3.2】成本函数：用于衡量参数w和b的效果,他是在全部训练集上来衡量

	【4】编程代码：【伪代码，不能运行，只是讲解帮助理解】
		# 以只有两个特征的样本为例：
	J=0
	dw1=0
	dw2=0
	db=0
	for i in range(1,m+1):	#遍历m个样本	#视频中的写法是 for i=1 to m 
		z[i]=w(t)*x[i]		#w(t)表示w的转置，x[i]表示第i个样本，不是滴i个特征
		a[i]=σ(z[i])
		J+=-y[i]log(a[i])+(1-y[i])log(1-a[i])
		dz[i]=a[i]-y[i]
		# 由于只有两个特征，下面的几行代码对特征不需要使用循环遍历，假如很多特征，就需要使用循环遍历了。
		# 但后面讲了for循环效率低下，要使用向量化方式
		dw1+=x1[i]dz[i]
		dw2+=x2[i]dz[i]
		db +=dz[i]
	J/=m	#即把m个损失函数之和也就是成本函数，除以m个样本
	dw1/=m 
	dw2/=m
	db/=m

	dw1=fy(j)/fy(w1)  #fy是求偏导符号，我自己简写的
	w1 = w1-α*dw1		
	w2 = w2-α*dw2			
	b  = b -α*db
	# 以上代码只对所有样本进行了一次迭代

	【5】疑问
	【5.1】可能会有的疑问：求出了成本函数J，为什么没有最小化成本函数J?
		答：最小化成本函数J的方法就是梯度下降法，上面的代码就是迭代就是在运用梯度下降法。

	【5.2】可能会有的疑问：单个样本的一次梯度迭代更新与所有样本的一次梯度迭代更新有什么不同？
		最新答案：随机梯度下降与梯度下降的折中方法，即每次计算一个batch的样本的损失函数。
		其中随机梯度下降法是随机优化某一条孙连数据的损失函数。
			'''更新W是在所有样本上更新的，因为成本函数是由所有样本的（预测值与目标值之差的绝对值）的平方，再求和得到的，
			所以大W的更新是所有小w一起更新。'''
		答：首先说明梯度指的是损失函数或者成本函数对每个权重求偏导数。每个特征对应一个权重（神经网络除外）。
		权重更新是每个权重分别更新，即w1,或者w2,...wn分别使用其对应的负梯度更新。
		所有样本的梯度迭代更新会用到单个样本的梯度，即所有样本的梯度是（所有单个样本的梯度和，除以样本个数m）。
		之所以单个样本讲到梯度更新是为了讲明白原理。当实际编程时，都是用到的所有样本的的梯度，
		单个样本梯度不更新，只相加得到所有样本总梯度，不会求单个样本的梯度更新的，
		即梯度更新是发生在求所有样本梯度和再除以样本总数m之后。
		【因此，在吴恩达视频中，Z=[z(1), z(1), ..., z(1)]=[w(T)x(1)+b, w(T)x(2)+b, ..., w(T)x(m)+b] 
		才会出现w(T)和所有样本都相乘，却并非每个样本的w(T)都不一样，因为所有样本都计算完毕才做一次迭代。】
		'''
		以下内容可不看，直接看"logistic回归_源代码.py"文件中的【0个显示for循环结构】
		X样本集，由所有样本以行向量形式组成，即每列是一个样本，每行是特征。
		Y是标签，是行向量，每个元素对应各个样本的标签。Y=[y(1),y(2),y(3),...,y(m)]
		W是权重，是n维列向量，n表示每个样本有n个特征
		Z=np.dot(w(T),X),
		a=σ(z),A=σ(Z)即A=[a(1),a(2),...,a(m)]即每个样本的预测值(激活值)组成了A，然后梯度更新会用到A，
		例如由单个非向量形式w1 = w1-α*x1*(a-y)，可知向量化形式为W = W - *X*(A-Y)，错误，
		正确方法应该用到hadamard乘积⊙，即每列对应元素相乘：W=W-α*X(T)⊙(A-Y)(T)。权重更新是每个权重分别更新
		'''
	=========================================================================================================
	【6】向量化
	向量化通常是消除你的代码中显示for循环语句的艺术
	关于向量化的例子和源代码，见 "dot_shape_reshape_向量化与for循环.py" 文件

	=========================================================================================================

	【7】np.dot()内积函数的用法
	【7.1】求向量内积时，函数np.dot(a,b)=a*b,向量a和向量b都不需要做任何转置，
		前提是：向量a和向量b其中某一个为行向量,另一个为列向量，且向量a和向量b的维度一致。
		当行*列，结果就是一个1*1矩阵
		当列*行，结果就是一个n*n矩阵

	【7.2】当a，b为矩阵时，a的列数要等于b的行数

	=========================================================================================================

	【8】broadcasting 广播
	 简单来说，广播就是使得
	 1.向量加减乘除常数时，把常数复制成向量的形式，使得他们可以相加减乘除  
		例如[1,2,3]+1=[1,2,3]+[1,1,1]=[2,3,4]
	 2.矩阵加减乘除向量时，把向量复制成矩阵的形式，使得他们可以相加减乘除	
		例如[[1,2,3],[4,5,6]]+[1,2,2]=[[1,2,3],[4,5,6]]+[[1,2,2],[1,2,2]]
	 3.。。。。
	----------------------------------------------------------------------
	import numpy as np
	A=np.array([[56.0, 0.0, 4.4, 68.0],
			   [1.2, 104.0, 52.0, 8.0],
			   [1.8, 135.0, 99.0, 0.9]])		   
	print('A=',A)

	cal=A.sum(axis=0)		#axis=0表示对矩阵A的每列求和；axis=1表示对矩阵A的每行求和
	print(cal)

	percentage=100*A/cal.reshape(1,4)	#reshape只是为了确保cal是1*4的矩阵，如果cal是，可不加，不是就加上。是不是都可以加上。
	print(percentage)
	--------------------------------------------------
	--------------------------------------------------
	# 当需要定义一维行向量和列向量时，务必指定行数和列数，避免生成既非行向量也非列向量的秩为1的数组
	# 如果你生成了秩为1的数组，可以使用a.reshape()函数来转换为行向量或者列向量
	# 注意维度问题，用array生成数组时，记得需要多打一对中括号[]把数组内容括起来
	# 可以使用断言assert语句来保证不生成秩为1的数组：
	a=np.array([1,2,3,4,5])		#(5,)
	assert a.shape==(1,5),'数组a不是(1,5)维的数组'		# AssertionError:数组a不是(1,5)维的数组
	
	a=np.array([[1,2,3,4,5]])
	assert a.shape==(1,5)，'数组a不是(1,5)维的数组'		#正常运行，不会出现AssertionError提示
	
	import numpy as np
	a=np.random.randn(5)	#生成5个随机高斯变量，储存在数组a中
	print(a)
	print(a.shape)		#(5,),秩为1的数组，他既不是行向量也不是列向量
	print(a.T)			#转置后也不变，还是秩为1的数组
	print(np.dot(a,a.T))

	a=np.random.randn(5,1)	#5*1的随机正态分布数组，列向量
	print(a.T)

	# 注意维度问题，用array生成数组时，记得需要多打一对中括号[]把数组内容括起来
	a=np.array([1,2,3])		#a.shape (3, ) 秩为1的数组
	b=np.array([[1,2,3]])	#b.shape (1, 3)
=================================================================================================
	【9】logistic损失函数与成本函数
	(y'和y"都表示预测值,也可就是激活值a，就是视频中的y帽)因为单引号双引号同一行换着使用才不会配对而把内容引起来
	----------------------------------------------
	【损失函数：】
	if y=1:	p(y|x)=y"	
	if y=0:	p(y|x)=1-y"
	则上两式子可合并为
		p(y|x)=y"^y * (1-y')^(1-y)

	log(p(y|x))=ylog(y') + (1-y)log(1-y")
			   =-L(y",y)
	即损失函数为
	L(a,y)= - [ylog(y') + (1-y)log(1-y")]
	因此，最小化损失函数就是最大化log(p(y|x))
	----------------------------------------------
	【成本函数：】
	对于单个样本来说：
		p(y|x)=y"^y * (1-y')^(1-y)
	假设所有样本独立同分布，所有这些样本的联合概率就是每个样本概率的乘积,整个训练集中标签的概率
		p(labels in target set)= ∏ p(y^(i)|x^(i)) # ∏表示求所有样本的乘积
	需要找到一组参数，使得给定样本的观测值概率最大，令这个概率最大化，等价于令其对数最大化：
		log [p(labels in target set)]= log{ ∏ p(y^(i)|x^(i))}
	训练集的标签出现的概率的对数等于
		log [p(labels in target set)]= ∑ log p(y^(i)|x^(i))	#对数()里面的乘积可以变成对数的相加
									 = -∑ L(y"^(i),y^(i))
	这样也就推导出了前面给出的logistic回归的成本函数J(w,b)=1/m *∑ L(y'^(i),y^(i)),这里加上了缩放常数因子1/m
														  =-1/m *∑ log p(y^(i)|x^(i))	
														  =-1/m *∑ log{y"^(i)^y^(i) * (1-y'^(i))^(1-y^(i))}
														  =-1/m *∑ {y^(i)log(y'^(i)) + (1-y^(i))log(1-y"^(i))}
					# 第【2】条里面写的成本函数：   J(w,b)=-1/m *∑[y(i)log(a(i))+(1-y(i))log(1-a(i))]   对上号了
	【最大似然估计】
	最大似然估计是给出了结果，求使得该结果出现可能性最大的一组参数的值。
		本例就是相当于给出J(w,b)，由于J(w,b)带负号，则相当于求使得-J(w,b)最大时
		（即J(w,b)最小时）的参数w,b的值。求最大最小这就变成了凸优化问题了
		更新迭代w,b,求出了最终的w,b那么模型就训练好了
