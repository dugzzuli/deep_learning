【1】步长为1时：
	假如图片大小(不填充或者填充后)为n*n,过滤器大小为f*f,那么卷积后得到的图像大小为 (n-f+1)*(n-f+1)
	这样做有两个缺点：
	(1)每次做卷积操作，你的图像就会缩小，你可能做了几次之后，你的图像就变得很小了，可能会缩小到只有1*1大小，你可能不想让你的图像
	   在每次识别边缘或识别其他特征时都缩小，这就是第一个缺点；
	(2)如果你注意角落边的像素，这个像素点只被一个输出所触碰或者说使用，因为它位于角。边界部分次之，比角上的点使用多一点。但是在
		中间的像素点，就会有许多过滤器会用到它。所以那些在角落或者边缘区域的像素点，在输出中采用较少，意味着丢掉了图像边缘位置的
		许多信息。
		
	(1)输出缩小：当我们建立深度神经网络时，你不希望每进行一步操作，图像就会缩小，比如你有100层的深层网络，
	如果图像每进一层都缩小的话，经过100层网络后，你会得到一个很小的图像。这是一个问题。
	(2)图像边界的大部分信息都丢失了。
	为了解决这2个问题，你可以在卷积操作之前，填充这幅图像。
	习惯上，你可以用0去填充。
	如果p是填充的数量,【假如图像n*n周边填充p=1个像素，则填充后图像变为(n+2)*(n+2)】，卷积后的输出为(n+2p-f+1)*(n+2p-f+1)
	
	至于选择填充多少像素，通常有两个选择，分别叫做 Valid卷积 和 Same卷积。
	Valid卷积：意味着不填充；即输入图像大小为n*n,输出大小为(n-f+1)*(n-f+1)
	Same卷积：意味着你填充后，你的输出大小和输入大小是一样的。当你填充p个像素点，输入大小n*n变为了输入大小(n+2p)*(n+2p)，
		输出变为了(n+2p-f+1)*(n+2p-f+1)。要使得输出大小和输入大小，则n=(n+2p-f+1),求解得p=(f-1)/2
		
【2】步长为s时：
	假如图片大小为n*n，过滤器大小为f*f,填充padding为p，步长stride为s，那么卷积后得到的图像大小为 [(n+2p-f)/s + 1] * [(n+2p-f)/s + 1]
	如果商(n+2p-f)/s不是一个整数，我们向下取整。即过滤器必须和覆盖图像的某部分，如果边界部分覆盖不了，那么就不进行卷积运算。

【3】互相关和卷积
	互相关就是咱们平时见到的不需要翻转的滤波器加权和。
	卷积就是把滤波器做副对角线"/"镜像翻转后，再求加权和。
	
	互相关和卷积有翻转的区别主要是因为：信号处理里面需要满足结合律A*B*C=A*(B*C)，这对于信号处理应用来说很好，但是对于深度神经网络来说，
	 它真的不重要。所以咱们平时说的卷积就是互相关的操作，即一般不对滤波器做镜像翻转，这样可以简化代码。所以不需要区分那么清楚，只需要
	 知道卷积就是求加权和就行。

【4】对多通道图像进行卷积
	例如对RGB三通道图像进行卷积：图片大小为6*6*3,过滤器大小为3*3*3。注意最后的3都表示通道数，必须一致，图像有几个通道，过滤器就得有
	几个通道。卷积过程是把过滤器的27个数与图片某位置的27个数求和，得到1个数字，整张6*6*3的图片卷积后得到4*4的二维图像。
	
	假如你想检测红色通道的水平边缘，过滤器就可以设置为：
		对应红色通道过滤器为[1,1,1;0,0,0;-1,-1,-1]，对应绿色和蓝色通道过滤器都为[0,0,0;0,0,0;0,0,0]
		最后得到的卷积后的图片是红色通道的水平边缘，绿色和蓝色通道全都被0过滤器消除了。
	假如你不关心是某个通道的水平边缘，即随便哪个通道的水平边缘都行，过滤器就可以设置为：
		对应红色、绿色和蓝色通道过滤器都为[1,1,1;0,0,0;-1,-1,-1]
	参数的选择不同，你就可以得到不同的特征检测器。
	
	假如你想检测水平边缘和垂直边缘这两个特征，怎么办？
		假设输入图片是6*6*3，你可以使用两个过滤器，一个检测水平边缘，另一个检测垂直边缘，把最后得到的两个4*4图片叠放在一起变成
		4*4*2的立方体。
	
	上面的例子中都是以RGB图像来解释说明，前两维叫做高height和宽width ，第三维叫做通道数channel；一般也可以把通道数叫做3维立方体的深度。

【5】
	卷积神经网络中，某一层的通道数等于前一层的过滤器数目。
	过滤器的通道数要和输入(指某一层的输入，而不是指最开始的输入)的通道数一样。
	
【6】池化层
	做池化操作时，输入通道数=输出通道数，因为对每个通道单独做了池化。
	一般情况下，池化层只有一个过滤器，这个过滤器就是最大值池化或平均值池化，即每个通道都是最大值池化或平均值池化。
	（池化层不像卷积层操作时不管输入图像是几个通道其输出均为一个通道，卷积操作的输出通道跟过滤器数目有关，有几个过滤器，就有几个通道）。
	需要注意一点：池化过程中没有需要学习的参数。执行反向传播时，反向传播没有参数适用于最大池化，只有这些设置过的超参数：filter_size，
	stride，max or average pooling,可能是手动设置的，也可能是通过交叉验证设置的。
	
	通常把有权重的层叫做一层，卷积层有权重(即过滤器)，池化层没有权重，只有一些超参数。所以通常把卷积层和池化层共同作为
	一个卷积，并标记为Layer1，后面的以此类推。你有时候也会看到卷积层和池化层各为一层的文献，这只是两种不同的标记术语。
	
	另一种情况是，一个或多个卷积层后面跟一个池化层。
	
